<head>
    <title>nothing is sacred for proxies</title>
    <link rel="stylesheet" href="/style.css">
    </head>
    <style>
    p {
            text-align: start;
    }
            
    </style>
    <body>
    <p>This article is based on the follow finding in cantina's
    <a href="https://cantina.xyz/competitions/e7af4986-183d-4764-8bd2-1d6b47f87d99">eigenlayer restaking contest</a>
    </p>
    <h2 id="-context-">Context</h2>
    <p>Eigenlayer ran a contests for their new slashing system during March 2025. The short summary of how it works is you have AVSs, which are on-chain contracts for verification and an off-chain network of Operators. Operators execute the service on behalf of the AVS and then post evidence of their execution on-chain to the AVS contracts. If the operator works properly, they are rewarded, but if they misbehave they get slashed and removed from the operator set. Operators have to stake ether to start working. Eventually when they want to withdrawal, they have to wait MIN_WITHDRAWAL_DELAY_BLOCKS on queue, which was set to 14 days. If they have funds in the queue and get slashed, not only their active stake is slashed but their queued ones as well.</p>
    <h2 id="-context-">Bug</h2>
    <p>Okay I gave you all the context u need. The way they track withdrawals is to snapshot any operator action that changes the queue, so either starting or finishing a withdrawal. When the operator has to be slashed the system looks at what the snapshot looked like MIN_WITHDRAWAL_DELAY_BLOCKS ago. </p>
    <p>The bug lies in the counterintuitive fact that for proxies, immutable variables are also upgradeable, once we presume governance CAN change MIN_WITHDRAWAL_DELAY_BLOCKS, we get a situation where before a queued withdrawal set before the change will misbehave. </p>
    <ol>
        <li>On the 15th day, an operator withdraws 500 of their 1000 shares.</li>
        <li>On the 25th day, the same operator withdraws another 100.</li>
        <li>On the 29th day, the operator completes the first withdrawal and gets the equivalent of 500 shares back.</li>
        <li>Starting from the 30th day, after the proposal is live, the operator starts misbehaving, for example by approving critically incorrect computation. The AVS then fully slashes their shares as punishment, and so they'll call slashOperator() in AllocationManager.sol, where it calls slashOperatorShares() in DelegationManager.sol.</li>
        <li>Inside this function, it calculates operatorSharesSlashed = operatorShares - operatorShares * newMag / prevMag which results in 400 shares.</li>
        <li>Then it calculates the shares to slash in queue: scaledSharesSlashedFromQueue = (curQueuedScaledShares - prevQueuedScaledShares) * (prevMax - newMax) / 1e18 This should be slashing the 100 shares in the queue, but thanks to the new WITHDRAWAL_DELAY, it will compare the current total cumulative shares in queue, 600, to the one 21 days prior, on the 9th, which was zero. Consequently the shares slashed from queue is set to 600 shares instead of 100.</li>
        <li>The operator is slashed, but the shares sent to the burn address is larger than what's queued. This excess loss is ultimately paid by the other operators in the strategy.</li>
        <img src="/eigenranks.jpeg" alt="eigenranks" width="640" height="360"> <figcaption>AVS hierarchy</figcaption>
    </ol>
    <p></p>
    <h2 id="-the-bug-">The governance entry point</h2>
    <p>In order to take part in governance, one must call the <code>participate()</code> function in the contract, <a href="https://github.com/Tapioca-DAO/tap-token-audit/blob/main/contracts/governance/twTAP.sol#L252">here</a>. Inside this function, there's a check for minimum duration of your lock, and a TAP <code>transferFrom</code> to the contract. <code>_computeMagnitude()</code> is called to calculate a magnitude of your voting power based around your lock duration. The contract then uses this magnitude to calculate a multiplier which is later required to get your final voting power. </p>
    <pre><code>
        <span class="hljs-keyword">function</span> <span class="hljs-title">participate(</span>
        address _participant,
        uint256 _amount,
        uint256 _duration
    ) external returns (uint256 tokenId) {
        require(_duration >= EPOCH_DURATION, "twTAP: Lock not a week");

        // Transfer TAP to this contract
        tapOFT.transferFrom(msg.sender, address(this), _amount);

        // Copy to memory
        TWAMLPool memory pool = twAML;

        uint256 magnitude = computeMagnitude(_duration, pool.cumulative);
        bool divergenceForce;
        uint256 multiplier = computeTarget(
            dMIN,
            dMAX,
            magnitude,
            pool.cumulative
        );
        . . .
        // Save twAML participation
        // Casts are safe: see struct definition
        uint256 votes = _amount * multiplier;
    </code></pre>
    <p>Have you caught the bug yet? Well here it is: there's no upper limit to your lock duration. As a consequence, you may join with a very small amount of tokens but with a lot of duration. Let's use my PoC case where you lock for <code>type(uint56).max - block.timestamp</code> seconds, which is about 2.3 billion years (did you know earth will be uninhabitable in 1.3 billion years? yeah its a lot of time). As a consequence, <code>computeMagnitude()</code> returns a blown up value, and later in the function when the average magnitude is calculated, it will easily surpass the divergence force. Future depositors will have their voting power forced to the lowest given the multiplier, calculated in the function below, will return the very minimum amount for the first few hundreds of participants.</p>
    <pre><code>    <span class="hljs-keyword">function</span> <span class="hljs-title">computeTarget(</span>
            uint256 _dMin,
            uint256 _dMax,
            uint256 _magnitude,
            uint256 _cumulative
        ) internal pure returns (uint256) {
            if (_cumulative == 0) {
                return _dMax;
            }
            uint256 target = (_magnitude * _dMax) / _cumulative;
            target = target > _dMax ? _dMax : target < _dMin ? _dMin : target;
            return target;
        }
    </code></pre>
    <p>The fix for this bug is quite simple: assign a maximum lock duration. What's interesting with this patch however is that a maximum amount is more subjective than one would imagine. This bug would technically (but with a much, much diminished impact) still happen if someone were to participate in the protocol for 50 years. I don't know much about Tapioca but I'm not confident anything we're using on the blockchain nowadays will be active for that long, so 50 years would effectively be "forever" in this case right? Knowing this I thought about how long would someone reasonable lock tokens in a contract for and I concluded 10 years would be a reasonable amount to submit. Anyway the devs chose a max duration of 100 years.</p>
    <p>Because of how the impact of the vulnerability differs based on how early you join the system, I coded and submitted 3 different tests. In hindsight, I went too far with such a large PoC. If I were to find this bug again I know for a fact I'd submit a single, most impactful PoC. A very detailed proof of how this impacted the protocol may have however helped this submission to be selected for report, which is always nice to see. The proof of concept tests can be found <a href="https://github.com/code-423n4/2023-07-tapioca-findings/issues/187">here</a> </p>
    <p>My submission was selected for report so thats cool. anyway, I wanted to make this post for a long time and it was harder then I anticipated. Turns out, its because I procastinated for so long I forgot how it even worked in the first place (oops). </p>
    <p></p>
    <a href="index">return</a>
    </body>
